{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy.random import random, shuffle\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoding(labels, bound):\n",
    "    result = np.zeros((labels.size, bound))\n",
    "    result[np.arange(labels.size), labels] = 1\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(vector):\n",
    "    '''\n",
    "    ReLU function Max{0, vector} for hidden layer\n",
    "    '''\n",
    "    new = np.empty_like(vector)\n",
    "    np.maximum(vector, 0, new)\n",
    "    return new\n",
    "\n",
    "def softmax(X):\n",
    "    '''\n",
    "    Softmax function for output layer\n",
    "    '''\n",
    "    return np.exp(X) / np.sum(np.exp(X), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_and_acc(X, Y, W, B):\n",
    "    N = X.shape[0]\n",
    "    Y_hat, _, _ = forward_prop(X, W[0], W[1], W[2], B[0], B[1], B[2])\n",
    "    loss = -np.sum(Y * np.log(Y_hat)) / N\n",
    "    acc = (np.argmax(Y_hat, axis=0) ==\n",
    "           np.argmax(Y, axis=0)).mean() * 100\n",
    "    return acc,loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_prop(X, w1, w2, w3, b1, b2, b3):\n",
    "    z2 = w1.dot(X) + b1\n",
    "    a2 = relu(z2)\n",
    "    z3 = w2.dot(a2) + b2\n",
    "    a3 = relu(z3)\n",
    "    z4 = w3.dot(a3) + b3\n",
    "    Y_hat = softmax(z4)\n",
    "\n",
    "    return Y_hat, [z2,z3,z4], [X,a2,a3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def d_relu(X):\n",
    "    # 0-1 \n",
    "    return np.where(X < 0, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_prop(Y_hat, y_ture, w, b, a, z):\n",
    "    N = Y_hat.shape[1]\n",
    "\n",
    "    delta3 = - (y_ture - Y_hat)\n",
    "    delta2 = (w[2].T.dot(delta3))*d_relu(z[1])\n",
    "    delta1 = (w[1].T.dot(delta2))*d_relu(z[0])\n",
    "    \n",
    "    g_w3 = delta3.dot(a[2].T)/ N\n",
    "    g_w2 = delta2.dot(a[1].T)/ N\n",
    "    g_w1 = delta1.dot(a[0].T)/ N\n",
    "\n",
    "    g_b1 = np.mean(delta1, axis=1, keepdims=True)\n",
    "    g_b2 = np.mean(delta2, axis=1, keepdims=True)\n",
    "    g_b3 = np.mean(delta3, axis=1, keepdims=True)\n",
    "    \n",
    "    return [g_w1,g_w2,g_w3], [g_b1,g_b2,g_b3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition of functions and parameters\n",
    "'''\n",
    "matrix format\n",
    "input matrx: sample_num*dimension_size\n",
    "weight matrix: input_dimension*output_dimension\n",
    "'''\n",
    "# for example\n",
    "EPOCH = 100\n",
    "batch_size = 100\n",
    "alpha = 0.1\n",
    "\n",
    "# Read all data from .pkl\n",
    "(train_images, train_labels, test_images, test_labels) = pickle.load(open('./mnist_data/data.pkl', 'rb'),encoding='latin1')\n",
    "train_images = np.array(train_images)\n",
    "train_labels = one_hot_encoding(np.array(train_labels), 10)\n",
    "test_images = np.array(test_images)\n",
    "test_labels = one_hot_encoding(np.array(test_labels), 10)\n",
    "\n",
    "### 1. Data preprocessing: normalize all pixels to [0,1) by dividing 256\n",
    "train_images = train_images/256\n",
    "test_images = test_images/256\n",
    "\n",
    "### 2. Weight initialization: Xavier\n",
    "n1 = train_images.shape[1]\n",
    "n2 = 300\n",
    "n3 = 100\n",
    "n4 = 10\n",
    "w1 = (np.random.random((n1,n2))-0.5)*np.sqrt(6/(n1+n2))\n",
    "w2 = (np.random.random((n2,n3))-0.5)*np.sqrt(6/(n2+n3))\n",
    "w3 = (np.random.random((n3,n4))-0.5)*np.sqrt(6/(n3+n4))\n",
    "W = (w1.T, w2.T, w3.T)\n",
    "b1 = np.zeros((n2,1))\n",
    "b2 = np.zeros((n3,1))\n",
    "b3 = np.zeros((n4, 1))\n",
    "B = (b1,b2,b3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Learning Rate=0.10\n",
      "Train Set: accuracy=90.0 loss=0.061889\n",
      "Test Set: accuracy=77.2 loss=0.931297\n",
      "Epoch 1, Learning Rate=0.10\n",
      "Train Set: accuracy=95.0 loss=0.030428\n",
      "Test Set: accuracy=85.5 loss=0.619770\n",
      "Epoch 2, Learning Rate=0.10\n",
      "Train Set: accuracy=95.0 loss=0.023327\n",
      "Test Set: accuracy=87.4 loss=0.527702\n",
      "Epoch 3, Learning Rate=0.10\n",
      "Train Set: accuracy=96.0 loss=0.019869\n",
      "Test Set: accuracy=88.7 loss=0.472922\n",
      "Epoch 4, Learning Rate=0.10\n",
      "Train Set: accuracy=97.0 loss=0.017535\n",
      "Test Set: accuracy=89.4 loss=0.432931\n",
      "Epoch 5, Learning Rate=0.10\n",
      "Train Set: accuracy=97.0 loss=0.015633\n",
      "Test Set: accuracy=90.3 loss=0.403744\n",
      "Epoch 6, Learning Rate=0.10\n",
      "Train Set: accuracy=98.0 loss=0.014049\n",
      "Test Set: accuracy=91.0 loss=0.379145\n",
      "Epoch 7, Learning Rate=0.10\n",
      "Train Set: accuracy=98.0 loss=0.012583\n",
      "Test Set: accuracy=91.4 loss=0.359601\n",
      "Epoch 8, Learning Rate=0.10\n",
      "Train Set: accuracy=99.0 loss=0.011296\n",
      "Test Set: accuracy=91.7 loss=0.343353\n",
      "Epoch 9, Learning Rate=0.10\n",
      "Train Set: accuracy=99.0 loss=0.010045\n",
      "Test Set: accuracy=91.7 loss=0.328950\n",
      "Epoch 10, Learning Rate=0.10\n",
      "Train Set: accuracy=99.0 loss=0.008968\n",
      "Test Set: accuracy=91.9 loss=0.316296\n",
      "Epoch 11, Learning Rate=0.10\n",
      "Train Set: accuracy=99.0 loss=0.008087\n",
      "Test Set: accuracy=92.3 loss=0.304570\n",
      "Epoch 12, Learning Rate=0.10\n",
      "Train Set: accuracy=100.0 loss=0.007288\n",
      "Test Set: accuracy=92.3 loss=0.293547\n",
      "Epoch 13, Learning Rate=0.10\n",
      "Train Set: accuracy=100.0 loss=0.006639\n",
      "Test Set: accuracy=92.4 loss=0.284817\n",
      "Epoch 14, Learning Rate=0.10\n",
      "Train Set: accuracy=100.0 loss=0.005989\n",
      "Test Set: accuracy=92.4 loss=0.276433\n",
      "Epoch 15, Learning Rate=0.10\n",
      "Train Set: accuracy=100.0 loss=0.005467\n",
      "Test Set: accuracy=92.9 loss=0.268981\n",
      "Epoch 16, Learning Rate=0.10\n",
      "Train Set: accuracy=100.0 loss=0.005009\n",
      "Test Set: accuracy=93.4 loss=0.262584\n",
      "Epoch 17, Learning Rate=0.10\n",
      "Train Set: accuracy=100.0 loss=0.004631\n",
      "Test Set: accuracy=93.6 loss=0.255802\n",
      "Epoch 18, Learning Rate=0.10\n",
      "Train Set: accuracy=100.0 loss=0.004293\n",
      "Test Set: accuracy=93.7 loss=0.250741\n",
      "Epoch 19, Learning Rate=0.10\n",
      "Train Set: accuracy=100.0 loss=0.003975\n",
      "Test Set: accuracy=93.7 loss=0.245991\n",
      "Epoch 20, Learning Rate=0.10\n",
      "Train Set: accuracy=100.0 loss=0.003684\n",
      "Test Set: accuracy=93.8 loss=0.241779\n",
      "Epoch 21, Learning Rate=0.10\n",
      "Train Set: accuracy=100.0 loss=0.003435\n",
      "Test Set: accuracy=94.0 loss=0.237429\n",
      "Epoch 22, Learning Rate=0.10\n",
      "Train Set: accuracy=100.0 loss=0.003229\n",
      "Test Set: accuracy=93.7 loss=0.234679\n",
      "Epoch 23, Learning Rate=0.10\n",
      "Train Set: accuracy=100.0 loss=0.003017\n",
      "Test Set: accuracy=93.7 loss=0.231232\n",
      "Epoch 24, Learning Rate=0.10\n",
      "Train Set: accuracy=100.0 loss=0.002843\n",
      "Test Set: accuracy=93.8 loss=0.228776\n",
      "Epoch 25, Learning Rate=0.10\n",
      "Train Set: accuracy=100.0 loss=0.002682\n",
      "Test Set: accuracy=94.0 loss=0.226386\n",
      "Epoch 26, Learning Rate=0.10\n",
      "Train Set: accuracy=100.0 loss=0.002520\n",
      "Test Set: accuracy=94.0 loss=0.224354\n",
      "Epoch 27, Learning Rate=0.10\n",
      "Train Set: accuracy=100.0 loss=0.002376\n",
      "Test Set: accuracy=94.0 loss=0.222279\n",
      "Epoch 28, Learning Rate=0.10\n",
      "Train Set: accuracy=100.0 loss=0.002254\n",
      "Test Set: accuracy=93.9 loss=0.221135\n",
      "Epoch 29, Learning Rate=0.10\n",
      "Train Set: accuracy=100.0 loss=0.002142\n",
      "Test Set: accuracy=94.1 loss=0.219307\n",
      "Epoch 30, Learning Rate=0.10\n",
      "Train Set: accuracy=100.0 loss=0.002031\n",
      "Test Set: accuracy=94.1 loss=0.218144\n",
      "Epoch 31, Learning Rate=0.10\n",
      "Train Set: accuracy=100.0 loss=0.001926\n",
      "Test Set: accuracy=94.1 loss=0.216669\n",
      "Epoch 32, Learning Rate=0.10\n",
      "Train Set: accuracy=100.0 loss=0.001837\n",
      "Test Set: accuracy=94.2 loss=0.215852\n",
      "Epoch 33, Learning Rate=0.10\n",
      "Train Set: accuracy=100.0 loss=0.001752\n",
      "Test Set: accuracy=94.2 loss=0.214996\n",
      "Epoch 34, Learning Rate=0.10\n",
      "Train Set: accuracy=100.0 loss=0.001670\n",
      "Test Set: accuracy=94.4 loss=0.213762\n",
      "Epoch 35, Learning Rate=0.10\n",
      "Train Set: accuracy=100.0 loss=0.001593\n",
      "Test Set: accuracy=94.4 loss=0.213304\n",
      "Epoch 36, Learning Rate=0.10\n",
      "Train Set: accuracy=100.0 loss=0.001514\n",
      "Test Set: accuracy=94.4 loss=0.212020\n",
      "Epoch 37, Learning Rate=0.10\n",
      "Train Set: accuracy=100.0 loss=0.001455\n",
      "Test Set: accuracy=94.3 loss=0.211664\n",
      "Epoch 38, Learning Rate=0.10\n",
      "Train Set: accuracy=100.0 loss=0.001385\n",
      "Test Set: accuracy=94.3 loss=0.211178\n",
      "Epoch 39, Learning Rate=0.10\n",
      "Train Set: accuracy=100.0 loss=0.001336\n",
      "Test Set: accuracy=94.4 loss=0.210214\n",
      "Epoch 40, Learning Rate=0.10\n",
      "Train Set: accuracy=100.0 loss=0.001279\n",
      "Test Set: accuracy=94.5 loss=0.209974\n",
      "Epoch 41, Learning Rate=0.10\n",
      "Train Set: accuracy=100.0 loss=0.001233\n",
      "Test Set: accuracy=94.6 loss=0.209161\n",
      "Epoch 42, Learning Rate=0.10\n",
      "Train Set: accuracy=100.0 loss=0.001187\n",
      "Test Set: accuracy=94.6 loss=0.208783\n",
      "Epoch 43, Learning Rate=0.10\n",
      "Train Set: accuracy=100.0 loss=0.001141\n",
      "Test Set: accuracy=94.6 loss=0.208421\n",
      "Epoch 44, Learning Rate=0.10\n",
      "Train Set: accuracy=100.0 loss=0.001107\n",
      "Test Set: accuracy=94.7 loss=0.207937\n",
      "Epoch 45, Learning Rate=0.10\n",
      "Train Set: accuracy=100.0 loss=0.001077\n",
      "Test Set: accuracy=94.7 loss=0.207471\n",
      "Epoch 46, Learning Rate=0.10\n",
      "Train Set: accuracy=100.0 loss=0.001041\n",
      "Test Set: accuracy=94.7 loss=0.206631\n",
      "Epoch 47, Learning Rate=0.10\n",
      "Train Set: accuracy=100.0 loss=0.001012\n",
      "Test Set: accuracy=94.6 loss=0.206746\n",
      "Epoch 48, Learning Rate=0.10\n",
      "Train Set: accuracy=100.0 loss=0.000980\n",
      "Test Set: accuracy=94.7 loss=0.206522\n",
      "Epoch 49, Learning Rate=0.10\n",
      "Train Set: accuracy=100.0 loss=0.000953\n",
      "Test Set: accuracy=94.8 loss=0.206113\n",
      "Epoch 50, Learning Rate=0.10\n",
      "Train Set: accuracy=100.0 loss=0.001190\n",
      "Test Set: accuracy=94.9 loss=0.202815\n",
      "Epoch 51, Learning Rate=0.01\n",
      "Train Set: accuracy=100.0 loss=0.001206\n",
      "Test Set: accuracy=94.9 loss=0.202702\n",
      "Epoch 52, Learning Rate=0.01\n",
      "Train Set: accuracy=100.0 loss=0.001212\n",
      "Test Set: accuracy=94.9 loss=0.202589\n",
      "Epoch 53, Learning Rate=0.01\n",
      "Train Set: accuracy=100.0 loss=0.001216\n",
      "Test Set: accuracy=94.9 loss=0.202502\n",
      "Epoch 54, Learning Rate=0.01\n",
      "Train Set: accuracy=100.0 loss=0.001217\n",
      "Test Set: accuracy=94.9 loss=0.202421\n",
      "Epoch 55, Learning Rate=0.01\n",
      "Train Set: accuracy=100.0 loss=0.001218\n",
      "Test Set: accuracy=94.9 loss=0.202347\n",
      "Epoch 56, Learning Rate=0.01\n",
      "Train Set: accuracy=100.0 loss=0.001217\n",
      "Test Set: accuracy=94.9 loss=0.202276\n",
      "Epoch 57, Learning Rate=0.01\n",
      "Train Set: accuracy=100.0 loss=0.001216\n",
      "Test Set: accuracy=94.9 loss=0.202221\n",
      "Epoch 58, Learning Rate=0.01\n",
      "Train Set: accuracy=100.0 loss=0.001214\n",
      "Test Set: accuracy=94.9 loss=0.202148\n",
      "Epoch 59, Learning Rate=0.01\n",
      "Train Set: accuracy=100.0 loss=0.001212\n",
      "Test Set: accuracy=94.9 loss=0.202110\n",
      "Epoch 60, Learning Rate=0.01\n",
      "Train Set: accuracy=100.0 loss=0.001210\n",
      "Test Set: accuracy=94.9 loss=0.202059\n",
      "Epoch 61, Learning Rate=0.01\n",
      "Train Set: accuracy=100.0 loss=0.001207\n",
      "Test Set: accuracy=94.9 loss=0.202012\n",
      "Epoch 62, Learning Rate=0.01\n",
      "Train Set: accuracy=100.0 loss=0.001205\n",
      "Test Set: accuracy=94.9 loss=0.201976\n",
      "Epoch 63, Learning Rate=0.01\n",
      "Train Set: accuracy=100.0 loss=0.001201\n",
      "Test Set: accuracy=94.9 loss=0.201940\n",
      "Epoch 64, Learning Rate=0.01\n",
      "Train Set: accuracy=100.0 loss=0.001199\n",
      "Test Set: accuracy=94.9 loss=0.201897\n",
      "Epoch 65, Learning Rate=0.01\n",
      "Train Set: accuracy=100.0 loss=0.001196\n",
      "Test Set: accuracy=94.9 loss=0.201871\n",
      "Epoch 66, Learning Rate=0.01\n",
      "Train Set: accuracy=100.0 loss=0.001192\n",
      "Test Set: accuracy=94.9 loss=0.201806\n",
      "Epoch 67, Learning Rate=0.01\n",
      "Train Set: accuracy=100.0 loss=0.001191\n",
      "Test Set: accuracy=94.9 loss=0.201760\n",
      "Epoch 68, Learning Rate=0.01\n",
      "Train Set: accuracy=100.0 loss=0.001186\n",
      "Test Set: accuracy=94.9 loss=0.201741\n",
      "Epoch 69, Learning Rate=0.01\n",
      "Train Set: accuracy=100.0 loss=0.001183\n",
      "Test Set: accuracy=94.9 loss=0.201673\n",
      "Epoch 70, Learning Rate=0.01\n",
      "Train Set: accuracy=100.0 loss=0.001180\n",
      "Test Set: accuracy=94.9 loss=0.201661\n",
      "Epoch 71, Learning Rate=0.01\n",
      "Train Set: accuracy=100.0 loss=0.001177\n",
      "Test Set: accuracy=94.9 loss=0.201614\n",
      "Epoch 72, Learning Rate=0.01\n",
      "Train Set: accuracy=100.0 loss=0.001173\n",
      "Test Set: accuracy=94.9 loss=0.201553\n",
      "Epoch 73, Learning Rate=0.01\n",
      "Train Set: accuracy=100.0 loss=0.001170\n",
      "Test Set: accuracy=94.9 loss=0.201531\n",
      "Epoch 74, Learning Rate=0.01\n",
      "Train Set: accuracy=100.0 loss=0.001167\n",
      "Test Set: accuracy=94.9 loss=0.201485\n",
      "Epoch 75, Learning Rate=0.01\n",
      "Train Set: accuracy=100.0 loss=0.001164\n",
      "Test Set: accuracy=94.9 loss=0.201449\n",
      "Epoch 76, Learning Rate=0.01\n",
      "Train Set: accuracy=100.0 loss=0.001160\n",
      "Test Set: accuracy=94.9 loss=0.201418\n",
      "Epoch 77, Learning Rate=0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Set: accuracy=100.0 loss=0.001156\n",
      "Test Set: accuracy=94.9 loss=0.201375\n",
      "Epoch 78, Learning Rate=0.01\n",
      "Train Set: accuracy=100.0 loss=0.001153\n",
      "Test Set: accuracy=94.9 loss=0.201335\n",
      "Epoch 79, Learning Rate=0.01\n",
      "Train Set: accuracy=100.0 loss=0.001150\n",
      "Test Set: accuracy=94.9 loss=0.201309\n",
      "Epoch 80, Learning Rate=0.01\n",
      "Train Set: accuracy=100.0 loss=0.001147\n",
      "Test Set: accuracy=94.9 loss=0.201247\n",
      "Epoch 81, Learning Rate=0.01\n",
      "Train Set: accuracy=100.0 loss=0.001143\n",
      "Test Set: accuracy=94.9 loss=0.201250\n",
      "Epoch 82, Learning Rate=0.01\n",
      "Train Set: accuracy=100.0 loss=0.001140\n",
      "Test Set: accuracy=94.9 loss=0.201207\n",
      "Epoch 83, Learning Rate=0.01\n",
      "Train Set: accuracy=100.0 loss=0.001136\n",
      "Test Set: accuracy=94.9 loss=0.201174\n",
      "Epoch 84, Learning Rate=0.01\n",
      "Train Set: accuracy=100.0 loss=0.001133\n",
      "Test Set: accuracy=94.9 loss=0.201141\n",
      "Epoch 85, Learning Rate=0.01\n",
      "Train Set: accuracy=100.0 loss=0.001130\n",
      "Test Set: accuracy=94.9 loss=0.201109\n",
      "Epoch 86, Learning Rate=0.01\n",
      "Train Set: accuracy=100.0 loss=0.001128\n",
      "Test Set: accuracy=94.9 loss=0.201091\n",
      "Epoch 87, Learning Rate=0.01\n",
      "Train Set: accuracy=100.0 loss=0.001125\n",
      "Test Set: accuracy=94.9 loss=0.201053\n",
      "Epoch 88, Learning Rate=0.01\n",
      "Train Set: accuracy=100.0 loss=0.001122\n",
      "Test Set: accuracy=94.9 loss=0.201035\n",
      "Epoch 89, Learning Rate=0.01\n",
      "Train Set: accuracy=100.0 loss=0.001118\n",
      "Test Set: accuracy=94.9 loss=0.201007\n",
      "Epoch 90, Learning Rate=0.01\n",
      "Train Set: accuracy=100.0 loss=0.001116\n",
      "Test Set: accuracy=94.9 loss=0.200968\n",
      "Epoch 91, Learning Rate=0.01\n",
      "Train Set: accuracy=100.0 loss=0.001113\n",
      "Test Set: accuracy=94.9 loss=0.200953\n",
      "Epoch 92, Learning Rate=0.01\n",
      "Train Set: accuracy=100.0 loss=0.001110\n",
      "Test Set: accuracy=94.9 loss=0.200925\n",
      "Epoch 93, Learning Rate=0.01\n",
      "Train Set: accuracy=100.0 loss=0.001107\n",
      "Test Set: accuracy=94.9 loss=0.200894\n",
      "Epoch 94, Learning Rate=0.01\n",
      "Train Set: accuracy=100.0 loss=0.001104\n",
      "Test Set: accuracy=94.9 loss=0.200891\n",
      "Epoch 95, Learning Rate=0.01\n",
      "Train Set: accuracy=100.0 loss=0.001102\n",
      "Test Set: accuracy=94.9 loss=0.200847\n",
      "Epoch 96, Learning Rate=0.01\n",
      "Train Set: accuracy=100.0 loss=0.001098\n",
      "Test Set: accuracy=94.9 loss=0.200832\n",
      "Epoch 97, Learning Rate=0.01\n",
      "Train Set: accuracy=100.0 loss=0.001095\n",
      "Test Set: accuracy=94.9 loss=0.200821\n",
      "Epoch 98, Learning Rate=0.01\n",
      "Train Set: accuracy=100.0 loss=0.001093\n",
      "Test Set: accuracy=94.9 loss=0.200817\n",
      "Epoch 99, Learning Rate=0.01\n",
      "Train Set: accuracy=100.0 loss=0.001091\n",
      "Test Set: accuracy=94.9 loss=0.200754\n"
     ]
    }
   ],
   "source": [
    "### 3. training of neural network\n",
    "loss = np.zeros((EPOCH))\n",
    "accuracy = np.zeros((EPOCH))\n",
    "training_size = train_images.shape[0]\n",
    "all_idx = list(range(training_size))\n",
    "lambda_ = 0.0005\n",
    "X_test, Y_test = test_images.T, test_labels.T\n",
    "\n",
    "for epoch in range(EPOCH):\n",
    "    print(\"Epoch %d, Learning Rate=%.2f\" % (epoch, alpha))\n",
    "    if epoch == 50:\n",
    "        alpha = 0.01\n",
    "    #shuffle(all_idx)\n",
    "    for batch_idx in np.array_split(all_idx, int(training_size / batch_size)):\n",
    "        # Forward propagation\n",
    "        X_trian, Y_train = train_images[batch_idx].T, train_labels[batch_idx].T\n",
    "\n",
    "        Y_hat, z, a = forward_prop(X_trian, *W, *B)\n",
    "        # Back propagation\n",
    "        #print(np.mean(Y_hat), np.max(Y_hat))\n",
    "        #break\n",
    "        g_w, g_b = back_prop(Y_hat,Y_train,W,B,a,z)\n",
    "        \n",
    "        # Gradient update\n",
    "        \n",
    "        w1 -= alpha * (g_w[0].T + lambda_*w1)\n",
    "        w2 -= alpha * (g_w[1].T + lambda_*w2)\n",
    "        w3 -= alpha * (g_w[2].T + lambda_*w3)\n",
    "        b1 -= alpha * g_b[0]\n",
    "        b2 -= alpha * g_b[1]\n",
    "        b3 -= alpha * g_b[2]\n",
    "\n",
    "    # After an epoch\n",
    "    # Testing for accuracy\n",
    "    result = loss_and_acc(X_trian, Y_train, W, B)\n",
    "    loss[epoch] = result[1]\n",
    "    print(\"Train Set: accuracy=%.1f loss=%f\" % result)\n",
    "    result = loss_and_acc(X_test, Y_test, W, B)\n",
    "    print(\"Test Set: accuracy=%.1f loss=%f\" % result)\n",
    "    accuracy[epoch] = result[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZEAAAD8CAYAAAC2PJlnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\njV8ywAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3128196470>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### 4. Plot\n",
    "# for example\n",
    "ax1 = plt.subplot(211)\n",
    "ax2 = plt.subplot(212)\n",
    "\n",
    "x = range(EPOCH)\n",
    "plt.sca(ax1)\n",
    "plt.plot(x, accuracy)\n",
    "plt.axis([0, 100, min(accuracy)-1, max(accuracy)+1])\n",
    "plt.ylabel('Accuracy')\n",
    "plt.sca(ax2)\n",
    "plt.plot(x, loss)\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
